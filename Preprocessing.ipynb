{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDFQuM3AQgQA"
      },
      "outputs": [],
      "source": [
        "!pip install nltk Sastrawi swifter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "hl0Pa8HJQsrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unduh korpus stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "7xxsLB59NOzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "daONFssBQstX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat variabel untuk dataset\n",
        "nama_file = '/content/dataset.csv'\n",
        "\n",
        "# Membaca dataset\n",
        "dataset = pd.read_csv(nama_file)"
      ],
      "metadata": {
        "id": "8SxMK5POQsvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "YShebBCdQsyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "id": "sEb_dqZ_RVJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset['text']"
      ],
      "metadata": {
        "id": "OKOAxnuLo-S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case Folding & cleaning text"
      ],
      "metadata": {
        "id": "P-BuatMJfLHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Pastikan nilai merupakan string\n",
        "    if isinstance(text, str):\n",
        "        #hapus nama\n",
        "        text = re.sub('@[^\\s]+', '', text)\n",
        "        #hapus tanda baca\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        #ganti karakter html dengan tanda petik\n",
        "        text = re.sub('<,*?>', '', text)\n",
        "        #pertimbangkan huruf dengan angka\n",
        "        text = re.sub('[^a-zA-Z]', ' ', text)\n",
        "        #ganti line baru dengan spasi\n",
        "        text = re.sub(\"\\n\", \" \", text)\n",
        "        #ubah ke huruf kecil\n",
        "        text = text.lower()\n",
        "        #hapus single char\n",
        "        text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
        "        #pisah dan gabungkan kata\n",
        "        text = ' '.join(text.split())\n",
        "        return text\n",
        "    else:\n",
        "        return \"\""
      ],
      "metadata": {
        "id": "U5j8MtYafK0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['clean'] = [clean_text(i) for i in data]\n",
        "data = dataset['clean']"
      ],
      "metadata": {
        "id": "X1geoxeJfJjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(10)"
      ],
      "metadata": {
        "id": "QTbxVanwfJpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "aO2AsTU6qEYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing"
      ],
      "metadata": {
        "id": "vLcfrMmPs9tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "regexp = RegexpTokenizer(r'\\w+|$[0-9]+|\\S+')\n",
        "dataset['Token'] = dataset['clean'].apply(regexp.tokenize)"
      ],
      "metadata": {
        "id": "bBZpAGzXs_up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "nM_JVGd9s_zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalisasi"
      ],
      "metadata": {
        "id": "bfHHierptv60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_word = pd.read_csv(\"normalisasi.csv\", encoding='latin1')\n",
        "\n",
        "normalized_word_dict = {}\n",
        "for index, row in normalized_word.iterrows():\n",
        "    if row[0] not in normalized_word_dict:\n",
        "        normalized_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_tern(document):\n",
        "    return [normalized_word_dict[tern] if tern in normalized_word_dict else tern for tern in document]\n",
        "\n",
        "dataset['normalisasi'] = dataset['Token'].apply(normalized_tern)\n",
        "dataset"
      ],
      "metadata": {
        "id": "YNUJwKwQs_3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hapus kata tidak perlu / remove stopword"
      ],
      "metadata": {
        "id": "nlukJ5JoJqY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ6v7KdDRGo_",
        "outputId": "ca7a044e-8c57-43b6-aab7-6e399ab1ebac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.csv  normalisasi.csv  sample_data  stopwords.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words('indonesian')\n",
        "txt_stopword = pd.read_csv(\"stopwords.txt\", names=[\"stopwords\"], header = None)\n",
        "\n",
        "stopword.extend([\"klo\", \"kl\", \"yg\", \"dgn\", \"dg\", \"g\", \"klrg\", \"dmn\", \"utk\", \"tdk\", \"sy\",\n",
        "                   \"gk\", \"jg\", \"nya\", \"bgt\", \"blh\", \"sih\", \"udh\", \"deh\", \"tp\",\n",
        "                   \"klo\", \"dll\", \"tpi\", \"krg\", \"enk\", \"dngn\", \"blg\"])\n",
        "\n",
        "stopword.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
        "stopword = set(stopword)\n",
        "\n",
        "def stopwords(text):\n",
        "  text = [word for word in text if word not in stopword]\n",
        "  return text\n",
        "dataset['Stopwords'] = dataset['normalisasi'].apply(stopwords)\n",
        "dataset"
      ],
      "metadata": {
        "id": "NP3ISsUdtvIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming / membakukan kata"
      ],
      "metadata": {
        "id": "FAHdhMowS6gM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "import swifter\n",
        "\n",
        "factory=StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "data = dataset['Stopwords']\n",
        "for doc in data:\n",
        "  for term in doc:\n",
        "    if term not in term_dict:\n",
        "      term_dict[term]=' '\n",
        "  print(len(term_dict))\n",
        "  print(\"...............\")\n",
        "\n",
        "for tern in term_dict:\n",
        "  term_dict[term] = stemmer.stem(term)\n",
        "  print(term,\":\",term_dict[term])\n",
        "\n",
        "  print(len(term_dict))\n",
        "  print(\"..................\")"
      ],
      "metadata": {
        "id": "ZbsZZht1tvNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stemmed_term(doc):\n",
        "  return [term_dict[term] for term in doc]\n",
        "\n",
        "dataset['Stemmer'] = dataset['Stopwords'].apply(get_stemmed_term)\n",
        "dataset"
      ],
      "metadata": {
        "id": "S-TDUSkaWAyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(dataset)\n",
        "data.to_excel ('teks_processing.xlsx', index = False, header=True)"
      ],
      "metadata": {
        "id": "lbVsjsQAWAl5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}